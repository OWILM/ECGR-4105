# HW7 Problem 2b — ResNet-10 (Regularization Experiments)
# Includes:
# Weight Decay (λ = 0.001)
# Dropout (p = 0.3)
# Batch Normalization 


import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time

# Use GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# CIFAR-10 preprocess
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))
])

train_set = torchvision.datasets.CIFAR10(root="./data", train=True,
                                         download=True, transform=transform)
test_set = torchvision.datasets.CIFAR10(root="./data", train=False,
                                        download=True, transform=transform)

train_loader = DataLoader(train_set, batch_size=128, shuffle=True)
test_loader  = DataLoader(test_set, batch_size=128, shuffle=False)


# RESIDUAL BLOCK
# Conv -> BN -> ReLU -> Conv -> BN -> +skip -> ReLU
class ResBlock(nn.Module):
    def __init__(self, channels, use_bn=True, dropout_p=0.0):
        super().__init__()
        self.use_bn = use_bn
        self.dropout_p = dropout_p

        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)

        if use_bn:
            self.bn1 = nn.BatchNorm2d(channels)
            self.bn2 = nn.BatchNorm2d(channels)

        if dropout_p > 0:
            self.dropout = nn.Dropout2d(p=dropout_p)

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        if self.use_bn: out = self.bn1(out)
        out = F.relu(out)

        out = self.conv2(out)
        if self.use_bn: out = self.bn2(out)

        if self.dropout_p > 0:
            out = self.dropout(out)

        out += identity
        out = F.relu(out)
        return out

# RESNET-10
class ResNet10(nn.Module):
    def __init__(self, channels=32, use_bn=True, dropout_p=0.0):
        super().__init__()

        self.conv_in = nn.Conv2d(3, channels, 3, padding=1)
        self.use_bn = use_bn

        if use_bn:
            self.bn_in = nn.BatchNorm2d(channels)

        self.blocks = nn.Sequential(
            *[ResBlock(channels, use_bn, dropout_p) for _ in range(10)]
        )

        self.fc = nn.Linear(channels, 10)

    def forward(self, x):
        out = self.conv_in(x)
        if self.use_bn: out = self.bn_in(out)
        out = F.relu(out)

        out = self.blocks(out)

        out = F.adaptive_avg_pool2d(out, 1)
        out = out.view(out.size(0), -1)
        return self.fc(out)


# Train + Evaluation

def train_one_epoch(model, loader, loss_fn, optimizer):
    model.train()
    total_loss, correct, total = 0, 0, 0

    for imgs, labels in loader:
        imgs, labels = imgs.to(device), labels.to(device)

        preds = model(imgs)
        loss = loss_fn(preds, labels)
        total_loss += loss.item()

        _, predicted = torch.max(preds, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return total_loss / len(loader), correct / total

def evaluate(model, loader, loss_fn):
    model.eval()
    total_loss, correct, total = 0, 0, 0

    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(device), labels.to(device)

            preds = model(imgs)
            loss = loss_fn(preds, labels)
            total_loss += loss.item()

            _, predicted = torch.max(preds, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return total_loss / len(loader), correct / total

# PARAMETER COUNT
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# REGULARIZATION MODE

REG_MODE = "weight_decay" 

if REG_MODE == "weight_decay":
    print("Running ResNet-10 with Weight Decay (0.001)")
    model = ResNet10(32, True, 0.0).to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)

elif REG_MODE == "dropout":
    print("Running ResNet-10 with Dropout (0.3)")
    model = ResNet10(32, True, 0.3).to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

elif REG_MODE == "batchnorm":
    print("Running ResNet-10 with BatchNorm")
    model = ResNet10(32, True, 0.0).to(device)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

print("Model parameters:", count_parameters(model))

loss_fn = nn.CrossEntropyLoss()


# TRAINING LOOP — 50 EPOCHS

epochs = 50
train_losses = []
val_losses = []
train_accs = []
val_accs = []

start = time.time()

for epoch in range(1, epochs + 1):
    tloss, tacc = train_one_epoch(model, train_loader, loss_fn, optimizer)
    vloss, vacc = evaluate(model, test_loader, loss_fn)

    train_losses.append(tloss)
    val_losses.append(vloss)
    train_accs.append(tacc)
    val_accs.append(vacc)

    print(f"Epoch {epoch}/{epochs} | "
          f"Train Loss: {tloss:.4f} | Val Loss: {vloss:.4f} | "
          f"Train Acc: {tacc:.4f} | Val Acc: {vacc:.4f}")

end = time.time()
print(f"\nTotal Training Time: {(end-start)/60:.2f} minutes")
print("Final Accuracy:", val_accs[-1])

# PLOTS
plt.figure(figsize=(14,5))

plt.subplot(1,2,1)
plt.plot(train_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.title(f"ResNet-10 ({REG_MODE}) — Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.subplot(1,2,2)
plt.plot(train_accs, label="Training Accuracy")
plt.plot(val_accs, label="Validation Accuracy")
plt.title(f"ResNet-10 ({REG_MODE}) — Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.show()

