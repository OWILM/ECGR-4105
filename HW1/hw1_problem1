import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# =========================================================================
#  Function Definitions
# =========================================================================

def compute_cost(X, y, theta):
    """
    Computes the cost (Mean Squared Error) for linear regression.
    """
    m = len(y)
    predictions = X.dot(theta)
    errors = np.subtract(predictions, y) 
    sqrErrors = np.square(errors)
    J = 1 / (2 * m) * np.sum(sqrErrors)
    return J

def gradient_descent(X, y, theta, alpha, iterations):
    """
    Performs gradient descent to learn theta by taking a specified number
    of gradient steps with a given learning rate.
    """
    m = len(y)
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        predictions = X.dot(theta)
        errors = np.subtract(predictions, y)
        sum_delta = (alpha / m) * X.transpose().dot(errors)
        theta -= sum_delta
        cost_history[i] = compute_cost(X, y, theta)

    return theta, cost_history

# =========================================================================
#  Problem 1
# =========================================================================

# 1. Load the dataset
df = pd.read_csv('D3.csv')
y = df.values[:, 3]
m = len(y)

# 2. Setup for iterating through each feature
features = ['X1', 'X2', 'X3']
colors = ['blue', 'green', 'red']
final_costs = {}

# 3. Loop through each feature, run regression, and plot results
for i, feature_name in enumerate(features):
    # Data Preparation for the current feature
    X_feature = df.values[:, i]
    X = np.vstack(X_feature)
    X_with_intercept = np.hstack((np.ones((m, 1)), X))

    # Gradient Descent
    theta = np.zeros(2)
    alpha = 0.001
    iterations = 1500

    theta, cost_history = gradient_descent(X_with_intercept, y, theta, alpha, iterations)
    final_cost = cost_history[-1]
    final_costs[feature_name] = final_cost

    # Plot 1: Regression Fit
    plt.scatter(X_feature, y, color=colors[i])
    plt.plot(X_feature, X_with_intercept.dot(theta), color='black')
    plt.title(f'Regression Model for {feature_name}')
    plt.xlabel(feature_name)
    plt.ylabel('Y')
    plt.show()

    # Plot 2: Loss Curve 
    plt.plot(range(iterations), cost_history, color='purple')
    plt.title(f'Loss Curve for {feature_name}')
    plt.xlabel('Number of Iterations')
    plt.ylabel('Cost (J)')
    plt.show()


# 4. Compare the final costs
print("\nFinal Comparison of Costs:")
for feature, cost in final_costs.items():
    print(f"{feature}: {cost:.4f}")

best_feature = min(final_costs, key=final_costs.get)
print(f"\nExplanatory variable with the lowest loss: {best_feature}")
